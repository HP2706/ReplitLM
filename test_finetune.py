# -*- coding: utf-8 -*-
"""finetune.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AvqWvKwT8IrqiGq7nFWpn7oNpq_qL5K4
"""

# for training
""" !pip install einops
!pip install torch
!pip install datasets
!pip install transformers
!pip install sentencepiece
!pip install tqdm
!pip install pynvml """

import einops
import torch
import torch.nn as nn
import numpy as np
import math
import tqdm.auto as tqdm
import numpy as np
from datasets import load_dataset
import time
import os
from transformers import AutoTokenizer, AutoModelForCausalLM
from torch.utils.data import DataLoader
from tqdm import tqdm
import torch.nn.functional as F
from replitLM_spec.modeling_mpt import MPTModel
from transformers import DataCollatorForLanguageModeling
import multiprocessing as mp
import json

import sys
sys.path.append("/content/ReplitLM/replitLM_spec")
sys.path.append("/content/ReplitLM")
print(sys.path)

from pynvml import *

from finetune import train, create_dataset, gpu_utilization

import os
os.environ["WANDB_API_KEY"] = "a3469eb2df23f67e4d6907ebacf50ffb4ee664f7"

with open('replitLM_spec/config2.json', 'r') as f:
    config = json.load(f)

print("config: ", config)

BATCH_SIZE = 16
train_dataloader, test_dataloader =  create_dataset(config, BATCH_SIZE)

print("dataloaders created")

train(config, train_dataloader, test_dataloader)