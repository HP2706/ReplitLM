{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HPfChJT34P8A"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/HansPeter/miniconda3/envs/replit-prune/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import einops\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import math\n",
        "import tqdm.auto as tqdm\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "import time\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "from replitLM_spec.modeling_mpt import MPTModel\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "import multiprocessing as mp\n",
        "import json\n",
        "import replitLM_spec\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNem0VgaYlXg",
        "outputId": "830d400b-cb30-40bb-bd9f-104f5516fe85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['/Users/HansPeter/Dev/coding_things/ReplitLM', '/Users/HansPeter/miniconda3/envs/replit-prune/lib/python311.zip', '/Users/HansPeter/miniconda3/envs/replit-prune/lib/python3.11', '/Users/HansPeter/miniconda3/envs/replit-prune/lib/python3.11/lib-dynload', '', '/Users/HansPeter/miniconda3/envs/replit-prune/lib/python3.11/site-packages', '/content/ReplitLM/replitLM_spec', '/content/ReplitLM']\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/ReplitLM/replitLM_spec\")\n",
        "sys.path.append(\"/content/ReplitLM\")\n",
        "print(sys.path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WB95KpN3Z-am"
      },
      "outputs": [],
      "source": [
        "from pynvml import *\n",
        "import json\n",
        "from finetune import train, create_dataset, gpu_utilization, upload_blob, upload_huggingface\n",
        "from replitLM_spec.configuration_mpt import MPTConfig\n",
        "from replitLM_spec.modeling_mpt import MPTModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vAagZ4f9cADt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_API_KEY\"] = \"a3469eb2df23f67e4d6907ebacf50ffb4ee664f7\"\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = \"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\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "74pi84CNneis"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import random\n",
        "from multiprocessing import Pool\n",
        "import multiprocessing as mp\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "from multiprocessing import Pool\n",
        "import json\n",
        "import multiprocessing as mp\n",
        "\n",
        "def generate_data(args):\n",
        "    start, end, sub_range_start, sub_range_end = args\n",
        "    dataset = []\n",
        "    PADDING_TOKEN = 0\n",
        "    random_numbers = set()\n",
        "    random_numbers.add(PADDING_TOKEN)\n",
        "\n",
        "\n",
        "    for _ in range(start, end):\n",
        "        random_number1, random_number2 = 0, 0\n",
        "\n",
        "        while random_number1 in random_numbers:\n",
        "            random_number1 = random.randint(sub_range_start, sub_range_end)\n",
        "        random_numbers.add(random_number1)\n",
        "\n",
        "        while random_number2 in random_numbers:\n",
        "            random_number2 = random.randint(sub_range_start, sub_range_end)\n",
        "        random_numbers.add(random_number2)\n",
        "\n",
        "        dataset.append((random_number1, random_number2))\n",
        "\n",
        "    return dataset\n",
        "\n",
        "def meta_generate_data(total_count, data_range):\n",
        "    # Divide the task into num_processes sub-tasks\n",
        "    num_processes = mp.cpu_count()\n",
        "    chunk_size = total_count // num_processes\n",
        "    sub_range_size = (data_range[1] - data_range[0] + 1) // num_processes\n",
        "    bounds = []\n",
        "    final_dataset = []\n",
        "    for i in range(num_processes):\n",
        "        start_idx = i * chunk_size\n",
        "        end_idx = (i + 1) * chunk_size\n",
        "        sub_range_start = data_range[0] + i * sub_range_size\n",
        "        sub_range_end = data_range[0] + (i + 1) * sub_range_size - 1\n",
        "        bound = (start_idx, end_idx, sub_range_start, sub_range_end)\n",
        "        final_dataset.extend(generate_data(bound))\n",
        "\n",
        "    # Combine the sub-datasets into the final dataset\n",
        "\n",
        "    with open('dataset.json', 'w') as f:\n",
        "        json.dump(final_dataset, f)\n",
        "\n",
        "    return final_dataset\n",
        "\n",
        "\n",
        "# Generate dataset\n",
        "if __name__ == '__main__':\n",
        "    dataset = meta_generate_data(10**4, (1, 10**8))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDRWuTCXngyX",
        "outputId": "d9e86bea-5c44-4a67-d01e-3f2f387fda9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "type <class 'list'>\n",
            "[[2758494 8185971]\n",
            " [5989946 8760309]\n",
            " [2734521 5356871]\n",
            " [ 838850 4730532]\n",
            " [9433899 2652111]\n",
            " [ 774698 9800062]\n",
            " [ 222283 9113566]\n",
            " [ 720304  169126]\n",
            " [9370190 3292994]\n",
            " [7883214 7078951]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def load_dataset(filename):\n",
        "    with open(filename, 'r') as f:\n",
        "        dataset = json.load(f)\n",
        "    print(\"type\", type(dataset))\n",
        "    dataset = np.array(dataset)\n",
        "    return dataset\n",
        "\n",
        "# Usage\n",
        "dataset_filename = 'dataset.json'\n",
        "loaded_dataset = load_dataset(dataset_filename)\n",
        "\n",
        "print(loaded_dataset[:10])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "U_H8Sd1JnjWr"
      },
      "outputs": [],
      "source": [
        "\n",
        "def calculate_bytes(dataset):\n",
        "    dataset = np.array(dataset)\n",
        "    size_of_single_int = np.dtype('int').itemsize\n",
        "    # Get the total number of number pairs (which is the same as the number of elements in the array)\n",
        "    total_number_pairs = dataset.shape\n",
        "    return size_of_single_int * total_number_pairs[0]*2\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9uV-AZninmxk"
      },
      "outputs": [],
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "from collections import Counter\n",
        "import re\n",
        "import os\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "import torch.optim as optim\n",
        "\n",
        "def train(dataset):\n",
        "    # Get vocab size\n",
        "    dataset = [f\"{x[0]}[SEP]{x[1]}\" for x in dataset]\n",
        "\n",
        "    # Initialize a tokenizer\n",
        "    tokenizer = Tokenizer(BPE())\n",
        "\n",
        "    # Initialize a trainer\n",
        "    trainer = BpeTrainer(special_tokens=[\"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "\n",
        "    # Train the tokenizer\n",
        "    tokenizer.train_from_iterator(dataset, trainer=trainer)\n",
        "\n",
        "    # Save the tokenizer\n",
        "    tokenizer.save(\"tokenizer.json\")\n",
        "\n",
        "    # To get vocab size\n",
        "    vocab_size = tokenizer.get_vocab_size()\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPJWLImPpv8n",
        "outputId": "ad3980bf-c209-4175-a90f-bae1f0a2977d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "30000"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trained_tokenizer = train(dataset)\n",
        "trained_tokenizer.get_vocab_size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jEXIP8ZrnjhA"
      },
      "outputs": [],
      "source": [
        "def encode(x, tokenizer, max_seq_len=7):\n",
        "    encoding = tokenizer.encode(f\"{x[0]}[SEP]{x[1]}\")\n",
        "    encoding.pad(max_seq_len)\n",
        "    return torch.tensor(encoding.ids)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "n7B2x6E-4W2n"
      },
      "outputs": [],
      "source": [
        "def collate_fn_inner(batch, tokenizer, max_seq_len=7):\n",
        "    input_ids = []\n",
        "    target_ids = []\n",
        "\n",
        "    for x in batch:  # Assuming each element in the batch is a tuple (word1, word2)\n",
        "\n",
        "        input_encoding = tokenizer.encode(str(x[0]))\n",
        "        target_encoding = tokenizer.encode(str(x[1]))\n",
        "\n",
        "        # Pad the encodings to max_seq_len\n",
        "        input_encoding.pad(max_seq_len)\n",
        "        target_encoding.pad(max_seq_len)\n",
        "\n",
        "        input_ids.append(torch.tensor(input_encoding.ids))\n",
        "        target_ids.append(torch.tensor(target_encoding.ids))\n",
        "\n",
        "    return {\n",
        "        'input_ids': torch.stack(input_ids),\n",
        "        'target_ids': torch.stack(target_ids)\n",
        "    }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return collate_fn_inner(batch, trained_tokenizer, max_seq_len=7)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fHLoGwLzYGc",
        "outputId": "ab182c2d-26f3-4a95-f117-021ff684307f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "160000"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "training_dataset = [encode(x, trained_tokenizer) for x in loaded_dataset]\n",
        "\n",
        "calculate_bytes(loaded_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZ5Lz_aSWSBr"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "M1d0JNKDmLxA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n",
            "Initializing Embedding with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'torch.nn.modules.sparse.Embedding'>\n",
            "Initializing Dropout with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Initializing LPLayerNorm with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "Initializing Linear with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'torch.nn.modules.linear.Linear'>\n",
            "Initializing BioLinear with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.bio.BioLinear'>\n",
            "Initializing Linear with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'torch.nn.modules.linear.Linear'>\n",
            "Initializing BioLinear with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.bio.BioLinear'>\n",
            "Initializing MultiheadAttention with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.attention.MultiheadAttention'>\n",
            "Initializing LPLayerNorm with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "Initializing Linear with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'torch.nn.modules.linear.Linear'>\n",
            "Initializing BioLinear with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.bio.BioLinear'>\n",
            "Initializing GELU with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'torch.nn.modules.activation.GELU'>\n",
            "Initializing Linear with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'torch.nn.modules.linear.Linear'>\n",
            "Initializing BioLinear with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.bio.BioLinear'>\n",
            "Initializing MPTMLP with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.blocks.MPTMLP'>\n",
            "Initializing Dropout with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Initializing Dropout with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Initializing MPTBlock with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.blocks.MPTBlock'>\n",
            "Initializing LPLayerNorm with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "Initializing Linear with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'torch.nn.modules.linear.Linear'>\n",
            "Initializing BioLinear with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.bio.BioLinear'>\n",
            "Initializing Linear with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'torch.nn.modules.linear.Linear'>\n",
            "Initializing BioLinear with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.bio.BioLinear'>\n",
            "Initializing MultiheadAttention with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.attention.MultiheadAttention'>\n",
            "Initializing LPLayerNorm with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "Initializing Linear with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'torch.nn.modules.linear.Linear'>\n",
            "Initializing BioLinear with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.bio.BioLinear'>\n",
            "Initializing GELU with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'torch.nn.modules.activation.GELU'>\n",
            "Initializing Linear with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'torch.nn.modules.linear.Linear'>\n",
            "Initializing BioLinear with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.bio.BioLinear'>\n",
            "Initializing MPTMLP with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.blocks.MPTMLP'>\n",
            "Initializing Dropout with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Initializing Dropout with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Initializing MPTBlock with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.blocks.MPTBlock'>\n",
            "Initializing LPLayerNorm with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "Initializing Linear with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'torch.nn.modules.linear.Linear'>\n",
            "Initializing BioLinear with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.bio.BioLinear'>\n",
            "Initializing Linear with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'torch.nn.modules.linear.Linear'>\n",
            "Initializing BioLinear with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.bio.BioLinear'>\n",
            "Initializing MultiheadAttention with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.attention.MultiheadAttention'>\n",
            "Initializing LPLayerNorm with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "Initializing Linear with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'torch.nn.modules.linear.Linear'>\n",
            "Initializing BioLinear with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.bio.BioLinear'>\n",
            "Initializing GELU with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'torch.nn.modules.activation.GELU'>\n",
            "Initializing Linear with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'torch.nn.modules.linear.Linear'>\n",
            "Initializing BioLinear with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.bio.BioLinear'>\n",
            "Initializing MPTMLP with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.blocks.MPTMLP'>\n",
            "Initializing Dropout with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Initializing Dropout with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Initializing MPTBlock with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.blocks.MPTBlock'>\n",
            "Initializing LPLayerNorm with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "Initializing Linear with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'torch.nn.modules.linear.Linear'>\n",
            "Initializing BioLinear with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.bio.BioLinear'>\n",
            "Initializing Linear with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'torch.nn.modules.linear.Linear'>\n",
            "Initializing BioLinear with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.bio.BioLinear'>\n",
            "Initializing MultiheadAttention with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.attention.MultiheadAttention'>\n",
            "Initializing LPLayerNorm with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "Initializing Linear with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'torch.nn.modules.linear.Linear'>\n",
            "Initializing BioLinear with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.bio.BioLinear'>\n",
            "Initializing GELU with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'torch.nn.modules.activation.GELU'>\n",
            "Initializing Linear with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'torch.nn.modules.linear.Linear'>\n",
            "Initializing BioLinear with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.bio.BioLinear'>\n",
            "Initializing MPTMLP with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.blocks.MPTMLP'>\n",
            "Initializing Dropout with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Initializing Dropout with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Initializing MPTBlock with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.blocks.MPTBlock'>\n",
            "Initializing ModuleList with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'torch.nn.modules.container.ModuleList'>\n",
            "Initializing Linear with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'torch.nn.modules.linear.Linear'>\n",
            "Initializing BioLinear with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.bio.BioLinear'>\n",
            "Initializing Linear with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'torch.nn.modules.linear.Linear'>\n",
            "Initializing BioLinear with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.bio.BioLinear'>\n",
            "Initializing LPLayerNorm with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "Initializing MPTModel with small_init_ function <function small_param_init_fn_ at 0x2984d77e0> \n",
            "module-type <class 'replitLM_spec.modeling_mpt.MPTModel'>\n",
            "Non-zero parameters: 120548\n",
            "Total parameters: 120548\n",
            "Sparsity Factor: 0.0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.0, 120548, 120548)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open('replitLM_spec/config2.json', 'r') as f:\n",
        "    config = json.load(f)\n",
        "mpt_config = MPTConfig(\n",
        "    d_model=config['d_model'],\n",
        "    n_heads=config['n_heads'],\n",
        "    n_layers=config['n_layers'],\n",
        "    expansion_ratio=config['expansion_ratio'],\n",
        "    max_seq_len= max([len(x) for x in training_dataset]),\n",
        "    vocab_size= trained_tokenizer.get_vocab_size(),\n",
        "    resid_pdrop=config['resid_pdrop'],\n",
        "    emb_pdrop=config['emb_pdrop'],\n",
        "    learned_pos_emb=config['learned_pos_emb'],\n",
        "    attn_config=config['attn_config'],\n",
        "    init_device=config['init_device'],\n",
        "    logit_scale=config['logit_scale'],\n",
        "    no_bias=config['no_bias'],\n",
        "    verbose=config['verbose'],\n",
        "    embedding_fraction=config['embedding_fraction'],\n",
        "    norm_type=config['norm_type'],\n",
        "    use_cache=config['use_cache'],\n",
        "    init_config=config['init_config']\n",
        ")\n",
        "\n",
        "\n",
        "BIOMODEL = MPTModel(mpt_config)\n",
        "BIOMODEL.calculate_sparsity()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4f8njaxT1qU",
        "outputId": "938a00ab-7eb6-4c1c-ae2b-6ec82edc94e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n",
            "1\n",
            "4\n"
          ]
        }
      ],
      "source": [
        "print(config['d_model'])\n",
        "print(config['n_heads'])\n",
        "print(config['n_layers'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "PeIhbZi0mL6m",
        "outputId": "623eff14-0d1e-4ff1-d899-fb9f3525dda8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "CR73PXhkZBWg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "<class 'replitLM_spec.bio.BioLinear'>\n",
            "<class 'replitLM_spec.bio.BioLinear'>\n",
            "<class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "<class 'replitLM_spec.bio.BioLinear'>\n",
            "<class 'replitLM_spec.bio.BioLinear'>\n",
            "<class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "<class 'replitLM_spec.bio.BioLinear'>\n",
            "<class 'replitLM_spec.bio.BioLinear'>\n",
            "<class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "<class 'replitLM_spec.bio.BioLinear'>\n",
            "<class 'replitLM_spec.bio.BioLinear'>\n",
            "<class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "<class 'replitLM_spec.bio.BioLinear'>\n",
            "<class 'replitLM_spec.bio.BioLinear'>\n",
            "<class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "<class 'replitLM_spec.bio.BioLinear'>\n",
            "<class 'replitLM_spec.bio.BioLinear'>\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for linear_layer in BIOMODEL.get_linear_layers():\n",
        "    print(type(linear_layer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/79 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Swap\n",
            "swap\n",
            "fold2: 1\n",
            "right.linear.weight.shape: torch.Size([12, 4])\n",
            "score: 0.0\n",
            "i=0, is norm\n",
            "fold2: 1\n",
            "right.linear.weight.shape: torch.Size([12, 4])\n",
            "score: 0.0\n",
            "i=0, is norm\n",
            "i=0, is norm\n",
            "left: <class 'NoneType'>\n",
            "right: <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "left: <class 'NoneType'>\n",
            "right: <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "left: <class 'NoneType'>\n",
            "right: <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "left: <class 'NoneType'>\n",
            "right: <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "left: <class 'NoneType'>\n",
            "right: <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "left: <class 'NoneType'>\n",
            "right: <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "left: <class 'NoneType'>\n",
            "right: <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "left: <class 'NoneType'>\n",
            "right: <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "left: <class 'NoneType'>\n",
            "right: <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "i=0, is norm\n",
            "left: <class 'NoneType'>\n",
            "right: <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "left: <class 'NoneType'>\n",
            "right: <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "left: <class 'NoneType'>\n",
            "right: <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "left: <class 'NoneType'>\n",
            "right: <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "left: <class 'NoneType'>\n",
            "right: <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "left: <class 'NoneType'>\n",
            "right: <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "left: <class 'NoneType'>\n",
            "right: <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "left: <class 'NoneType'>\n",
            "right: <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "left: <class 'NoneType'>\n",
            "right: <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "i=0, is norm\n",
            "left: <class 'NoneType'>\n",
            "right: <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "left: <class 'NoneType'>\n",
            "right: <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "left: <class 'NoneType'>\n",
            "right: <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "left: <class 'NoneType'>\n",
            "right: <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "left: <class 'NoneType'>\n",
            "right: <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "left: <class 'NoneType'>\n",
            "right: <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "left: <class 'NoneType'>\n",
            "right: <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "left: <class 'NoneType'>\n",
            "right: <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "left: <class 'NoneType'>\n",
            "right: <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "i=0, is norm\n",
            "left: <class 'NoneType'>\n",
            "right: <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "left: <class 'NoneType'>\n",
            "right: <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "left: <class 'NoneType'>\n",
            "right: <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "left: <class 'NoneType'>\n",
            "right: <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "left: <class 'NoneType'>\n",
            "right: <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "left: <class 'NoneType'>\n",
            "right: <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "left: <class 'NoneType'>\n",
            "right: <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "left: <class 'NoneType'>\n",
            "right: <class 'replitLM_spec.norm.LPLayerNorm'>\n",
            "left: <class 'NoneType'>\n",
            "right: <class 'replitLM_spec.norm.LPLayerNorm'>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'LPLayerNorm' object has no attribute 'out_fold'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mif\u001b[39;00m (step\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m1\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     49\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m Swap\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m   BIOMODEL\u001b[39m.\u001b[39;49mrelocate()\n\u001b[1;32m     52\u001b[0m \u001b[39m# Update parameters\u001b[39;00m\n\u001b[1;32m     53\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
            "File \u001b[0;32m~/Dev/coding_things/ReplitLM/replitLM_spec/modeling_mpt.py:559\u001b[0m, in \u001b[0;36mMPTModel.relocate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    556\u001b[0m num_linear \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(linears)\n\u001b[1;32m    557\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_linear\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m    558\u001b[0m     \u001b[39m#print(i)\u001b[39;00m\n\u001b[0;32m--> 559\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrelocate_i(i)\n",
            "File \u001b[0;32m~/Dev/coding_things/ReplitLM/replitLM_spec/modeling_mpt.py:542\u001b[0m, in \u001b[0;36mMPTModel.relocate_i\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[39m# normal swap + the first res swap\u001b[39;00m\n\u001b[0;32m--> 542\u001b[0m top_id_head \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_top_id_head(i, top_k\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtop_k)\n\u001b[1;32m    543\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(top_id_head\u001b[39m.\u001b[39mitem()):\n\u001b[1;32m    544\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelocate_ij_head(i,j)\n",
            "File \u001b[0;32m~/Dev/coding_things/ReplitLM/replitLM_spec/modeling_mpt.py:430\u001b[0m, in \u001b[0;36mMPTModel.get_top_id_head\u001b[0;34m(self, i, top_k)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mres_swap[\u001b[39m0\u001b[39m]:\n\u001b[1;32m    429\u001b[0m     \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mres_swap:\n\u001b[0;32m--> 430\u001b[0m         score \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_score(p)\n\u001b[1;32m    431\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    432\u001b[0m     score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_score(i)\n",
            "File \u001b[0;32m~/Dev/coding_things/ReplitLM/replitLM_spec/modeling_mpt.py:388\u001b[0m, in \u001b[0;36mMPTModel.get_score\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(left, nn\u001b[39m.\u001b[39mLayerNorm) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(left, LPLayerNorm):\n\u001b[1;32m    387\u001b[0m     left \u001b[39m=\u001b[39m linears[i\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m--> 388\u001b[0m fold \u001b[39m=\u001b[39m left\u001b[39m.\u001b[39;49mout_fold\n\u001b[1;32m    389\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfold2: \u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    390\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mright.linear.weight.shape: \u001b[39m\u001b[39m{\u001b[39;00mleft\u001b[39m.\u001b[39mlinear\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m~/miniconda3/envs/replit-prune/lib/python3.11/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'LPLayerNorm' object has no attribute 'out_fold'"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "dataloader = DataLoader(loaded_dataset, batch_size=128, collate_fn=collate_fn, shuffle=True)\n",
        "EPOCH = 1000\n",
        "\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "BIOMODEL\n",
        "optimizer = optim.Adam(BIOMODEL.parameters(), lr=0.001, weight_decay=0.0)\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "steps = int(40000)\n",
        "log = 100\n",
        "lamb = 1e-3\n",
        "swap_log = 1000\n",
        "\n",
        "step = 0\n",
        "\n",
        "for i in range(EPOCH):\n",
        "    for batch in tqdm(dataloader):\n",
        "        step += 1\n",
        "        # Move model to training mode\n",
        "        BIOMODEL.train()\n",
        "\n",
        "        if step == int(steps*1/4):\n",
        "          lamb *= 10\n",
        "\n",
        "        if step == int(steps*3/4):\n",
        "            lamb *= 10\n",
        "\n",
        "        # Move data to device\n",
        "        # dict = {k: v.to('cuda') for k, v in batch.items()}\n",
        "        #batch = {k: v.to('cpu') for k, v in batch.items()}\n",
        "        # Forward pass\n",
        "        logits , _ = BIOMODEL(input_ids=batch['input_ids'], attention_mask=batch['target_ids'])\n",
        "\n",
        "        logits = logits.view(-1, logits.size(-1))  # shape [batch_size * seq_len, vocab_size]\n",
        "        labels = batch['target_ids'].view(-1)  # shape [batch_size * seq_len]\n",
        "        loss = loss_func(logits, labels)\n",
        "\n",
        "        cc = BIOMODEL.get_cc(no_penalize_last=False)\n",
        "        total_loss = loss + lamb*cc\n",
        "\n",
        "        # Zero out gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        if (step+1) % 1 == 0:\n",
        "          print(\"\\n Swap\")\n",
        "          BIOMODEL.relocate()\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f\"LOSS {loss.item()}, BIO_loss {cc.item()}, TOTAL LOSS {total_loss.item()}, EPOCH {i}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3On7bRYLMkd"
      },
      "outputs": [],
      "source": [
        "def get_model_size_in_bytes(model: nn.Module) -> int:\n",
        "    total_size = 0\n",
        "    count = 0\n",
        "    for param in model.parameters():\n",
        "        # Number of elements multiplied by the number of bytes for each element\n",
        "        total_size += param.numel() * param.element_size()\n",
        "        count += param.numel()\n",
        "    return total_size\n",
        "\n",
        "get_model_size_in_bytes(BIOMODEL)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
