{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/HansPeter/miniconda3/envs/llm-experiments/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'replitLM_spec.modelling_mpt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[1;32m     15\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mreplitLM_spec\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodelling_mpt\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpy\u001b[39;00m \u001b[39mimport\u001b[39;00m MPTModel\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'replitLM_spec.modelling_mpt'"
     ]
    }
   ],
   "source": [
    "\n",
    "import einops\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "import tqdm.auto as tqdm\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/HansPeter/Dev/coding_things/ReplitLM/replitLM_spec\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(os.getcwd())\n",
    "from modelling_mpt import MPTModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading dataset\n",
    "\n",
    "t0 = time.time()\n",
    "token = \"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\"\n",
    "\n",
    "# full dataset (3TB of data)\n",
    "ds = load_dataset(\"bigcode/the-stack-dedup\", split=\"train[:0.01%]\", use_auth_token=token)\n",
    "print(time.time() - t0)\n",
    "\n",
    "\n",
    "#preparing dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"replit/replit-code-v1-3b\", trust_remote_code=True)\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True)\n",
    "\n",
    "ds = ds.map(tokenize_batch, batched=True)\n",
    "ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels']) # Assuming 'labels' is your target variable\n",
    "data_loader = DataLoader(ds, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "model = MPTModel.from_pretrained(\"replit/replit-code-v1-3b\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "steps = int(1000)\n",
    "log = 100\n",
    "batch_size = 64 #128\n",
    "lamb = 1e-3\n",
    "swap_log = int(1e6) #1000\n",
    "plot_log = 1000\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    raise ValueError('No GPU found, please run with --cuda')\n",
    "\n",
    "for step in tqdm(range(steps)):\n",
    "    if step == int(steps * 1 / 4):\n",
    "        lamb *= 10\n",
    "    if step == int(steps * 3 / 4):\n",
    "        lamb *= 10\n",
    "\n",
    "    for batch in data_loader:\n",
    "        # Move the batch to the GPU if available\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "        loss = F.cross_entropy(outputs.logits, batch['labels'])\n",
    "        cc = model.get_cc(no_penalize_last=False)\n",
    "        total_loss = loss + lamb*cc\n",
    "        total_loss.backward()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "    if step % log == 0:\n",
    "        print(\"step = %d | train loss: %.2e | train last: %.2e | test loss %.2e | test last: %.2e | cc: %.2e \"%(step, loss.detach().numpy(), loss_last.detach().numpy(), loss_test.detach().numpy(), loss_test_last.detach().numpy(), cc.detach().numpy()))\n",
    "\n",
    "    if (step+1) % swap_log == 0:\n",
    "        model.relocate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
